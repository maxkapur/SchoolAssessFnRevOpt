{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization problems in explicit form\n",
    "\n",
    "SciPy's optimization routines treat the objective function as a black box, so previously, I was able to define a funny loss function that had lots of if/then logic in it depending on whether a student was correctly classified.\n",
    "\n",
    "However, by adding decision variables that correspond to the absolute difference between each misclassified students' score and the cutoff, we can create an explicit optimization problem. Moreover, the problem becomes a linear program if we assume $\\rho = 1$, which (according to the results) seems to be a reasonable assumption.\n",
    "\n",
    "Here I provide these optimization problems and validate them by showing that the results agree with those found in the main notebook.\n",
    "\n",
    "I use Julia's JuMP package, which provides a more legible syntax for working with optimization problems that have many decision variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP\n",
    "using Ipopt\n",
    "using DataFrames\n",
    "using CSV\n",
    "ENV[\"COLUMNS\"] = 100;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary admissions outcomes\n",
    "\n",
    "For the binary admissions data, where $y_n \\in \\left\\{0, 1\\right\\}$ represents whether a student was admitted, we used an $L_1$ penalty function that adds up the absolute difference $\\xi$ between the score and cutoff for any student who is misclassified. Using standard optimization tricks, it's not difficult to show that the problem formulated previously is equivalent to the following:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\underset{a, \\rho, p, \\xi}{\\text{minimize}}\\quad&  \\sum_{n} \\xi_n \\\\[.75em]\n",
    "\\text{subject to}\\quad & \\Bigl(\\sum_{t} a_t X_{nt}^\\rho\\Bigr)^{1/\\rho} \\leq p + \\xi_n, \\quad\\forall n:  y_n = 0 \\\\[.5em]\n",
    " & \\Bigl(\\sum_{t} a_t X_{nt}^\\rho\\Bigr)^{1/\\rho}\\geq p - \\xi_n, \\quad \\forall n: y_n = 1 \\\\[.5em]\n",
    " & \\sum_t a_t = 1, \\quad a,\\,p,\\,\\xi \\geq 0, \\quad \\rho \\leq 1\n",
    "\\end{align}$$\n",
    "\n",
    "Let's verify this empirically.\n",
    "\n",
    "Read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Column1</th><th>admit</th><th>gre</th><th>gpa</th><th>rank</th><th>feat1</th><th>feat2</th><th>x1</th><th>x2</th></tr><tr><th></th><th>Int64</th><th>Int64</th><th>Int64</th><th>Float64</th><th>Int64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>5 rows × 9 columns</p><tr><th>1</th><td>0</td><td>0</td><td>380</td><td>3.61</td><td>3</td><td>-210.867</td><td>-0.242558</td><td>0.0533333</td><td>0.473333</td></tr><tr><th>2</th><td>1</td><td>1</td><td>660</td><td>3.67</td><td>3</td><td>69.1331</td><td>-0.539923</td><td>0.703333</td><td>0.276667</td></tr><tr><th>3</th><td>2</td><td>1</td><td>800</td><td>4.0</td><td>1</td><td>209.135</td><td>1.31509</td><td>1.0</td><td>0.876667</td></tr><tr><th>4</th><td>3</td><td>1</td><td>640</td><td>3.19</td><td>4</td><td>49.1315</td><td>-1.52472</td><td>0.626667</td><td>0.06</td></tr><tr><th>5</th><td>4</td><td>0</td><td>520</td><td>2.93</td><td>4</td><td>-70.8687</td><td>-1.40029</td><td>0.253333</td><td>0.12</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccccc}\n",
       "\t& Column1 & admit & gre & gpa & rank & feat1 & feat2 & x1 & x2\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & Int64 & Int64 & Float64 & Int64 & Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 0 & 0 & 380 & 3.61 & 3 & -210.867 & -0.242558 & 0.0533333 & 0.473333 \\\\\n",
       "\t2 & 1 & 1 & 660 & 3.67 & 3 & 69.1331 & -0.539923 & 0.703333 & 0.276667 \\\\\n",
       "\t3 & 2 & 1 & 800 & 4.0 & 1 & 209.135 & 1.31509 & 1.0 & 0.876667 \\\\\n",
       "\t4 & 3 & 1 & 640 & 3.19 & 4 & 49.1315 & -1.52472 & 0.626667 & 0.06 \\\\\n",
       "\t5 & 4 & 0 & 520 & 2.93 & 4 & -70.8687 & -1.40029 & 0.253333 & 0.12 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m5×9 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Column1 \u001b[0m\u001b[1m admit \u001b[0m\u001b[1m gre   \u001b[0m\u001b[1m gpa     \u001b[0m\u001b[1m rank  \u001b[0m\u001b[1m feat1     \u001b[0m\u001b[1m feat2     \u001b[0m\u001b[1m x1        \u001b[0m\u001b[1m x2       \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64   \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int64 \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64   \u001b[0m\u001b[90m Float64  \u001b[0m\n",
       "─────┼──────────────────────────────────────────────────────────────────────────────────\n",
       "   1 │       0      0    380     3.61      3  -210.867   -0.242558  0.0533333  0.473333\n",
       "   2 │       1      1    660     3.67      3    69.1331  -0.539923  0.703333   0.276667\n",
       "   3 │       2      1    800     4.0       1   209.135    1.31509   1.0        0.876667\n",
       "   4 │       3      1    640     3.19      4    49.1315  -1.52472   0.626667   0.06\n",
       "   5 │       4      0    520     2.93      4   -70.8687  -1.40029   0.253333   0.12"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binaryinp = DataFrame(CSV.File(\"binaryinp.csv\"))\n",
    "first(binaryinp, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulate the optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimize (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function optimize()\n",
    "    N = size(binaryinp)[1]\n",
    "    T = 2\n",
    "    X = Array(binaryinp[!, [:x1, :x2]])\n",
    "    \n",
    "    model = Model(Ipopt.Optimizer)\n",
    "    set_optimizer_attribute(model, \"print_level\", 0)\n",
    "    \n",
    "    @variable(model, 0 ≤ a[1:T], start = 0.5)\n",
    "    @variable(model, ρ ≤ 1, start = 0.5)\n",
    "    @variable(model, 0 ≤ p, start = 0.5)\n",
    "    @variable(model, 0 ≤ ξ[1:N])\n",
    "    \n",
    "    @objective(model, Min, sum(ξ))\n",
    "\n",
    "    for n in 1:N\n",
    "        if binaryinp[n, :admit] < 0.5            # If rejected\n",
    "            @NLconstraint(model, sum(a[t] * X[n, t]^ρ for t in 1:T) ≤ (p + ξ[n])^ρ)\n",
    "        else                                     # If admitted\n",
    "            @NLconstraint(model, sum(a[t] * X[n, t]^ρ for t in 1:T) ≥ (p - ξ[n])^ρ)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    @constraint(model, Regularization, sum(a) == 1)\n",
    "    \n",
    "    optimize!(model)\n",
    "    \n",
    "    return value.(a), value.(ρ), value.(p)\n",
    "end  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the results agree with those we found before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************************************************************************\n",
      "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
      " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
      "         For more information visit https://github.com/coin-or/Ipopt\n",
      "******************************************************************************\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.5167223716264429, 0.4832776283735572], 1.0, 0.5942642068479204)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, ρ, p = optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic admissions outcomes\n",
    "\n",
    "For probabilistic admissions data, $y_n \\in \\left[0, 1\\right]$ represents each student's admissions probability. We weighted the $L_1$ penalty by how far our admissions \"guess\" (zero or one) was from the true probability. Again, using standard optimization tricks, the following problem is equivalent:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\underset{a, \\rho, p, \\xi, d}{\\text{minimize}}\\quad&  \\sum_{n} d_n \\\\[.75em]\n",
    "\\text{subject to}\\quad & \\Bigl(\\sum_{t} a_t X_{nt}^\\rho\\Bigr)^{1/\\rho} - p = \\xi_n \\\\[.5em]\n",
    "\t\t\t   & d_n \\geq \\xi_n\\left(1 - y_n\\right), \\quad d_n \\geq -\\xi_n y_n  \\\\[.5em]\n",
    "\t\t\t   & \\sum_t a_t = 1, \\quad a,\\,p,\\,d \\geq 0, \\quad \\rho \\leq 1\n",
    "\\end{align}$$\n",
    "The new variable\n",
    "$$\\begin{align}\n",
    "d_n &\\equiv \n",
    "\\begin{cases}\n",
    " \\xi_n \\left(1-y_n\\right), &  ~~\\text{ if } u\\left(X_{n.}\\right)- p = \\xi_n \\geq 0 \\\\\n",
    "-\\xi_n y_n, & ~~\\text{ if } u\\left(X_{n.} \\right) - p = \\xi_n < 0\n",
    " \\end{cases} \\\\[.5em]\n",
    "  &= \\max \\left\\{ \\xi_n (1 - y_n), -\\xi_n y_n \\right\\}\n",
    "\\end{align}$$\n",
    " represents how \"correct\" the guess is; notice no sign constraint on $\\xi$. Again assuming $\\rho = 1$ yields an LP. \n",
    " \n",
    "Let's check this.\n",
    "\n",
    "Read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>chance</th><th>x1</th><th>x2</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>5 rows × 3 columns</p><tr><th>1</th><td>0.92</td><td>0.965</td><td>0.5975</td></tr><tr><th>2</th><td>0.76</td><td>0.6675</td><td>0.1325</td></tr><tr><th>3</th><td>0.72</td><td>0.455</td><td>0.1725</td></tr><tr><th>4</th><td>0.8</td><td>0.6425</td><td>0.47</td></tr><tr><th>5</th><td>0.65</td><td>0.3825</td><td>0.1675</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& chance & x1 & x2\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 0.92 & 0.965 & 0.5975 \\\\\n",
       "\t2 & 0.76 & 0.6675 & 0.1325 \\\\\n",
       "\t3 & 0.72 & 0.455 & 0.1725 \\\\\n",
       "\t4 & 0.8 & 0.6425 & 0.47 \\\\\n",
       "\t5 & 0.65 & 0.3825 & 0.1675 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m5×3 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m chance  \u001b[0m\u001b[1m x1      \u001b[0m\u001b[1m x2      \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\n",
       "─────┼───────────────────────────\n",
       "   1 │    0.92   0.965    0.5975\n",
       "   2 │    0.76   0.6675   0.1325\n",
       "   3 │    0.72   0.455    0.1725\n",
       "   4 │    0.8    0.6425   0.47\n",
       "   5 │    0.65   0.3825   0.1675"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probinp = DataFrame(CSV.File(\"probinp.csv\"))\n",
    "probinp[1:5, [:chance, :x1, :x2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimize (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function optimize()\n",
    "    N = size(probinp)[1]\n",
    "    T = 2\n",
    "    X = Array(probinp[!, [:x1, :x2]])\n",
    "    y = Array(probinp[!, :chance])\n",
    "    \n",
    "    model = Model(Ipopt.Optimizer)\n",
    "    set_optimizer_attribute(model, \"print_level\", 0)\n",
    "    \n",
    "    @variable(model, 0 ≤ a[1:T], start = 0.5)\n",
    "    @variable(model, ρ ≤ 1, start = 0.5)\n",
    "    @variable(model, 0 ≤ p, start = 0.5)\n",
    "    @variable(model, ξ[1:N])\n",
    "    @variable(model, 0 ≤ d[1:N])\n",
    "    \n",
    "    @objective(model, Min, sum(d))\n",
    "\n",
    "    @NLconstraint(model, Error[n in 1:N], sum(a[t] * X[n, t]^ρ for t in 1:T)^(1/ρ) == p + ξ[n])\n",
    "    \n",
    "    # This rearrangement yields a different solution that appears to be marginally better. \n",
    "    # But I am suspicious of it because it yields ρ ≈ 0, which yields low error in this equation\n",
    "    # regardless of p and ξ.\n",
    "\n",
    "    # @NLconstraint(model, Error[n in 1:N], sum(a[t] * X[n, t]^ρ for t in 1:T) == (p + ξ[n])^ρ)    \n",
    "    \n",
    "    @constraint(model, ErrorAdmit[n in 1:N], d[n] ≥ ξ[n] * (1 - y[n]))\n",
    "    @constraint(model, ErrorReject[n in 1:N], d[n] ≥ -ξ[n] * y[n])\n",
    "    @constraint(model, Regularization, sum(a) == 1)\n",
    "    \n",
    "    optimize!(model)\n",
    "    \n",
    "    @show objective_value(model)\n",
    "    \n",
    "    return value.(a), value.(ρ), value.(p)\n",
    "end  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective_value(model) = 18.95084762846951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.6213902658254695, 0.37860973417453053], 0.9771543582677937, 0.35469610680026437)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, ρ, p = optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results agree with SciPy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
